"""
Module d'Analyse IA - Tanmia Scraper MVP v1.9
PATCH: Merge unifi√© page+fichiers + Focus Organisation/Emails

CHANGELOG v1.9:
- PATCH: merge_all_content() - fusionne page+fichiers AVANT appel IA
- PATCH: Prompt simplifi√© focus Organisation + Emails (copie exacte)
- PATCH: D√©duplication + normalisation emails
- CONSERV√â: Structure v1.8 qui fonctionne
"""
import re
import json
from typing import Dict, List, Optional


# ============================================================================
# PATCH v1.9: MERGE UNIFI√â PAGE + FICHIERS
# ============================================================================

def merge_all_content(data: Dict) -> str:
    """
    Fusionne page + fichiers en un seul texte pour analyse.
    
    Args:
        data: Dict avec texte_complet et fichiers_attaches
    
    Returns:
        Texte unifi√© pour analyse IA
    """
    parts = []
    
    # 1. Contenu page
    texte_page = data.get('texte_complet', '')
    if texte_page:
        parts.append("=== CONTENU PAGE WEB ===")
        parts.append(texte_page)
    
    # 2. Contenu fichiers (si pars√©s)
    for f in data.get('fichiers_attaches', []):
        contenu = f.get('contenu_texte', '')
        if contenu:
            nom = f.get('nom', 'fichier_inconnu')
            parts.append(f"\n=== FICHIER: {nom} ===")
            # Limiter taille
            if len(contenu) > 4000:
                contenu = contenu[:4000] + "...[tronqu√©]"
            parts.append(contenu)
    
    return '\n\n'.join(parts)


def normalize_and_dedup_emails(emails_page: List[str], emails_fichiers: List[str]) -> List[str]:
    """
    Normalise et d√©duplique tous les emails.
    
    Returns:
        Liste unique, lowercase, tri√©e
    """
    all_emails = set()
    
    for email in emails_page + emails_fichiers:
        if email and '@' in str(email):
            normalized = email.strip().lower()
            if re.match(r'^[\w\.\-\+]+@[\w\.\-]+\.\w{2,}$', normalized):
                all_emails.add(normalized)
    
    return sorted(all_emails)


# ============================================================================
# FORMATAGE FICHIERS POUR PROMPTS (v1.8 enrichi)
# ============================================================================

def format_fichiers_for_prompt(fichiers: List[Dict], include_content: bool = True) -> str:
    """
    Formate les fichiers attach√©s avec leur contenu pour le prompt IA.
    
    Args:
        fichiers: Liste de dicts {nom, url, type, contenu_texte, emails_fichier}
        include_content: Si True, inclut le contenu textuel des fichiers
    
    Returns:
        String format√© pour le prompt
    """
    if not fichiers or len(fichiers) == 0:
        return "Aucun fichier attach√©."
    
    lines = []
    
    for idx, f in enumerate(fichiers, 1):
        nom = f.get('nom', 'Sans nom')
        type_f = f.get('type', 'inconnu').upper()
        contenu = f.get('contenu_texte', '')
        emails = f.get('emails_fichier', [])
        
        # En-t√™te fichier
        lines.append(f"\n--- FICHIER {idx}: {nom} ({type_f}) ---")
        
        # Emails trouv√©s dans le fichier
        if emails:
            lines.append(f"Emails trouv√©s dans ce fichier: {', '.join(emails)}")
        
        # Contenu textuel (si disponible et demand√©)
        if include_content and contenu:
            # Tronquer si trop long
            if len(contenu) > 3000:
                contenu = contenu[:3000] + "...[contenu tronqu√©]"
            lines.append(f"Contenu extrait:\n{contenu}")
        elif not contenu:
            lines.append("(Contenu non disponible - fichier non pars√©)")
    
    return '\n'.join(lines)


def get_all_emails_from_files(fichiers: List[Dict]) -> List[str]:
    """
    R√©cup√®re tous les emails trouv√©s dans les fichiers.
    
    Args:
        fichiers: Liste de fichiers avec emails_fichier
    
    Returns:
        Liste d'emails uniques
    """
    all_emails = []
    for f in fichiers:
        all_emails.extend(f.get('emails_fichier', []))
    return list(set(all_emails))


# ============================================================================
# PROMPTS PROFESSIONNELS - GEMINI 2.5 PRO (v1.8)
# ============================================================================

GEMINI_ANALYSIS_PROMPT = """Tu es un expert analyste sp√©cialis√© dans l'extraction d'informations structur√©es √† partir d'annonces professionnelles et de leurs documents annexes (TDR, cahiers des charges) dans le secteur du d√©veloppement international au Maroc.

CONTEXTE:
Tu analyses des opportunit√©s publi√©es sur Tanmia.ma avec leurs fichiers attach√©s (PDF, DOC).
Ces documents contiennent souvent des informations cruciales: emails de contact, d√©tails de mission, profils recherch√©s.

DONN√âES √Ä ANALYSER:

URL: {url}
M√©tadonn√©es:
- Titre: {titre}
- Organisation: {organisation}
- Date: {date}

EMAILS D√âJ√Ä EXTRAITS DES FICHIERS (v1.8):
{emails_from_files}

FICHIERS ATTACH√âS ET LEUR CONTENU:
{fichiers_attaches}

TEXTE DE LA PAGE WEB:
{texte_complet}

T√ÇCHE:
Analyse LE TEXTE DE LA PAGE et LE CONTENU DES FICHIERS pour extraire:

0. ORGANISATION / EMPLOYEUR
   - Cherche dans le texte ET dans les fichiers attach√©s
   - Les TDR mentionnent souvent l'organisation commanditaire
   - Retourne le NOM PROPRE (1-4 mots)

1. EMAILS DE CONTACT (PRIORIT√â ABSOLUE)
   - FUSIONNE les emails de la page ET ceux des fichiers
   - Les emails d√©j√† extraits des fichiers sont fournis ci-dessus
   - Cherche aussi dans le texte de la page
   - Formats: standard, avec espaces, obfusqu√©s
   - NE MANQUE AUCUN EMAIL - c'est l'info la plus critique

2. SECTEUR D'ACTIVIT√â (LISTE FERM√âE)
   Choisis UN parmi: "Sant√©", "√âducation", "Environnement", "Humanitaire", "D√©veloppement", "Gouvernance", "Droits humains", "Autre"

3. TYPE D'OPPORTUNIT√â
   Parmi: "CDI", "CDD", "Freelance", "Mission courte", "Appel d'offres"

4. LOCALISATION
   Ville(s), "National", ou "Non sp√©cifi√©"

5. R√âSUM√â PROFESSIONNEL (ENRICHI v1.8)
   - 2-3 phrases max (80 mots)
   - UTILISE les infos des fichiers attach√©s (TDR, cahiers des charges)
   - Mentionne: objectif, profil recherch√©, budget si mentionn√©, dur√©e mission
   - Exemple: "Mission d'√©valuation finale sur 3 mois. Budget indicatif: 50 000 MAD. Profil expert M&E requis."

6. MOTS-CL√âS TECHNIQUES
   - 5-8 mots-cl√©s du texte ET des fichiers
   - Inclus comp√©tences sp√©cifiques mentionn√©es dans les TDR
   - Ajoute "TDR d√©taill√©", "Cahier des charges" si fichiers importants pr√©sents

CONTRAINTES:
- Analyse TOUS les contenus (page + fichiers)
- Priorise les emails (fusionne toutes sources)
- JSON strict sans markdown

FORMAT R√âPONSE:
{{
    "organisation": "Nom organisation",
    "emails": ["email1@org.ma", "email2@org.ma"],
    "secteur": "UN des 8 choix",
    "type_opportunite": "Type exact",
    "localisation": "Ville(s)",
    "resume": "Synth√®se incluant infos des fichiers attach√©s.",
    "mots_cles": ["mot1", "mot2", "mot3", "mot4", "mot5"]
}}

ANALYSE MAINTENANT.
"""


# ============================================================================
# PROMPTS PROFESSIONNELS - CLAUDE (v1.8)
# ============================================================================

CLAUDE_ANALYSIS_PROMPT = """Tu es un expert analyste sp√©cialis√© dans l'extraction d'informations √† partir d'annonces professionnelles ET de leurs documents annexes (TDR, cahiers des charges) dans le secteur humanitaire au Maroc.

<contexte>
Tu analyses des opportunit√©s Tanmia.ma avec leurs fichiers attach√©s pars√©s.
Les TDR et cahiers des charges contiennent souvent les informations les plus pr√©cises: emails directs, d√©tails budg√©taires, profils exacts recherch√©s.
</contexte>

<donnees>
URL: {url}

M√©tadonn√©es:
- Titre: {titre}
- Organisation: {organisation}
- Date: {date}

EMAILS EXTRAITS DES FICHIERS (pr√©-extraction v1.8):
{emails_from_files}

FICHIERS ATTACH√âS AVEC CONTENU:
{fichiers_attaches}

TEXTE PAGE WEB:
{texte_complet}
</donnees>

<instructions>
Analyse le texte de la page ET le contenu des fichiers attach√©s.

0. ORGANISATION
   - Cherche dans page ET fichiers (les TDR mentionnent le commanditaire)
   - NOM PROPRE court (1-4 mots)

1. EMAILS (PRIORIT√â ABSOLUE)
   - FUSIONNE: emails page + emails fichiers (fournis ci-dessus)
   - Les TDR contiennent souvent l'email direct du contact
   - D√©tecte tous formats (standard, obfusqu√©s)
   - Retourne TOUS les emails trouv√©s

2. SECTEUR
   STRICTEMENT parmi: "Sant√©", "√âducation", "Environnement", "Humanitaire", "D√©veloppement", "Gouvernance", "Droits humains", "Autre"

3. TYPE D'OPPORTUNIT√â
   Parmi: "CDI", "CDD", "Freelance", "Mission courte", "Appel d'offres"

4. LOCALISATION
   Ville(s), "National", ou "Non sp√©cifi√©"

5. R√âSUM√â (ENRICHI v1.8)
   - 2-3 phrases maximum
   - EXPLOITE les infos des fichiers:
     * Budget si mentionn√©
     * Dur√©e mission
     * Livrables attendus
     * Qualifications sp√©cifiques
   - Exemple: "√âvaluation programme VIH/SIDA sur 45 jours. Budget: 80 000 MAD. Expert sant√© publique avec 10 ans d'exp√©rience requis. TDR complet disponible."

6. MOTS-CL√âS
   - 5-8 mots-cl√©s de la page ET des fichiers
   - Comp√©tences techniques des TDR
   - Ajoute "TDR d√©taill√©" ou "Cahier des charges" si pertinent
</instructions>

<format_reponse>
JSON strict sans backticks:
{{
    "organisation": "Nom",
    "emails": ["email1@domain.com", "email2@domain.com"],
    "secteur": "UN des 8 choix",
    "type_opportunite": "Type",
    "localisation": "Ville(s)",
    "resume": "Synth√®se avec infos fichiers.",
    "mots_cles": ["mot1", "mot2", "mot3", "mot4", "mot5"]
}}
</format_reponse>

ANALYSE MAINTENANT.
"""


# ============================================================================
# FONCTION PRINCIPALE D'ANALYSE
# ============================================================================

def analyze_opportunity(
    data: Dict, 
    api_key: str, 
    ai_type: str = "claude"
) -> Dict:
    """
    Analyse une opportunit√© avec l'IA (v1.8 avec contenu fichiers).
    
    Args:
        data: Donn√©es scrap√©es incluant fichiers avec contenu
        api_key: Cl√© API
        ai_type: "claude" ou "gemini"
    
    Returns:
        Dict avec analyse structur√©e
    """
    if ai_type == "claude":
        return analyze_with_claude(data, api_key)
    else:
        return analyze_with_gemini(data, api_key)


def create_fallback_analysis(data: Dict) -> Dict:
    """Cr√©e une analyse fallback si l'IA √©choue."""
    texte = data.get('texte_complet', '')
    
    # Emails de la page
    emails = extract_emails_regex(texte)
    
    # Emails des fichiers (v1.8)
    emails_files = data.get('emails_from_files', [])
    all_emails = list(set(emails + emails_files))
    
    # R√©sum√©
    resume = texte[:200] + "..." if len(texte) > 200 else texte
    
    fichiers = data.get('fichiers_attaches', [])
    if fichiers:
        nb_parses = sum(1 for f in fichiers if f.get('contenu_texte'))
        resume += f" ({len(fichiers)} fichier(s), {nb_parses} pars√©(s))"
    
    return {
        'organisation': data.get('organisation', 'Non sp√©cifi√©'),
        'emails': all_emails,
        'secteur': 'Autre',
        'type_opportunite': 'Non d√©termin√©',
        'localisation': 'Non sp√©cifi√©',
        'resume': resume,
        'mots_cles': []
    }


def analyze_with_gemini(data: Dict, api_key: str) -> Dict:
    """Analyse avec Gemini (v1.9 - texte unifi√©)."""
    try:
        import google.generativeai as genai
        
        genai.configure(api_key=api_key)
        model = genai.GenerativeModel(
            model_name='gemini-2.0-flash',
            generation_config={
                'temperature': 0.1,
                'top_p': 0.9,
                'max_output_tokens': 1024,
            }
        )
        
        # PATCH v1.9: Merge tout le contenu
        texte_unifie = merge_all_content(data)
        
        # Limiter taille
        if len(texte_unifie) > 12000:
            texte_unifie = texte_unifie[:12000] + "...[tronqu√©]"
        
        # Emails pr√©-extraits (regex)
        emails_page = extract_emails_regex(data.get('texte_complet', ''))
        emails_fichiers = []
        for f in data.get('fichiers_attaches', []):
            emails_fichiers.extend(f.get('emails_fichier', []))
        
        # PATCH v1.9: Prompt simplifi√©
        prompt = f"""Tu es un extracteur de donn√©es. Analyse le texte ci-dessous et extrait UNIQUEMENT:

1. ORGANISATION: Le nom EXACT et COMPLET de l'entit√© qui publie cette offre.
   - Copie le nom tel qu'il appara√Æt, sans abr√©ger ni reformuler.
   - Cherche: Association, Fondation, ONG, Direction, Minist√®re, etc.

2. EMAILS: Tous les emails de contact trouv√©s.

R√àGLES STRICTES:
- NE PAS inventer. Si non trouv√©, mettre "Non sp√©cifi√©".
- NE PAS r√©sumer ou interpr√©ter. Copier EXACTEMENT.
- R√©pondre UNIQUEMENT en JSON valide, sans markdown.

=== TEXTE √Ä ANALYSER ===

{texte_unifie}

=== FIN DU TEXTE ===

R√©ponds avec ce JSON uniquement:
{{"organisation": "...", "emails": ["...", "..."], "secteur": "Autre", "type_opportunite": "Offre", "localisation": "Non sp√©cifi√©", "resume": "...", "mots_cles": []}}
"""
        
        response = model.generate_content(prompt)
        response_text = clean_json_response(response.text.strip())
        result = json.loads(response_text)
        
        # PATCH v1.9: Fusionner et normaliser emails
        emails_ia = result.get('emails', [])
        all_emails = normalize_and_dedup_emails(emails_page + emails_fichiers, emails_ia)
        result['emails'] = all_emails
        
        return result
        
    except Exception as e:
        print(f"‚ùå Erreur Gemini: {e}")
        return create_fallback_analysis(data)


def analyze_with_claude(data: Dict, api_key: str) -> Dict:
    """Analyse avec Claude (v1.9 - texte unifi√©)."""
    try:
        import anthropic
        
        client = anthropic.Anthropic(api_key=api_key)
        
        # PATCH v1.9: Merge tout le contenu
        texte_unifie = merge_all_content(data)
        
        # Limiter taille
        if len(texte_unifie) > 12000:
            texte_unifie = texte_unifie[:12000] + "...[tronqu√©]"
        
        # Emails pr√©-extraits (regex)
        emails_page = extract_emails_regex(data.get('texte_complet', ''))
        emails_fichiers = []
        for f in data.get('fichiers_attaches', []):
            emails_fichiers.extend(f.get('emails_fichier', []))
        
        # PATCH v1.9: Prompt simplifi√©
        prompt = f"""Tu es un extracteur de donn√©es. Analyse le texte ci-dessous et extrait UNIQUEMENT:

1. ORGANISATION: Le nom EXACT et COMPLET de l'entit√© qui publie cette offre.
   - Copie le nom tel qu'il appara√Æt, sans abr√©ger ni reformuler.
   - Cherche: Association, Fondation, ONG, Direction, Minist√®re, etc.

2. EMAILS: Tous les emails de contact trouv√©s.

R√àGLES STRICTES:
- NE PAS inventer. Si non trouv√©, mettre "Non sp√©cifi√©".
- NE PAS r√©sumer ou interpr√©ter. Copier EXACTEMENT.
- R√©pondre UNIQUEMENT en JSON valide, sans markdown.

=== TEXTE √Ä ANALYSER ===

{texte_unifie}

=== FIN DU TEXTE ===

R√©ponds avec ce JSON uniquement:
{{"organisation": "...", "emails": ["...", "..."], "secteur": "Autre", "type_opportunite": "Offre", "localisation": "Non sp√©cifi√©", "resume": "...", "mots_cles": []}}
"""
        
        message = client.messages.create(
            model="claude-sonnet-4-20250514",
            max_tokens=1024,
            temperature=0.1,
            messages=[{"role": "user", "content": prompt}]
        )
        
        response_text = clean_json_response(message.content[0].text.strip())
        result = json.loads(response_text)
        
        # PATCH v1.9: Fusionner et normaliser emails
        emails_ia = result.get('emails', [])
        all_emails = normalize_and_dedup_emails(emails_page + emails_fichiers, emails_ia)
        result['emails'] = all_emails
        
        return result
        
    except Exception as e:
        print(f"‚ùå Erreur Claude: {e}")
        return create_fallback_analysis(data)


# ============================================================================
# UTILITAIRES
# ============================================================================

def clean_json_response(text: str) -> str:
    """Nettoie la r√©ponse IA pour extraire le JSON."""
    if "```json" in text:
        text = text.split("```json")[1].split("```")[0]
    elif "```" in text:
        text = text.split("```")[1].split("```")[0]
        if text.strip().startswith("json"):
            text = text.strip()[4:]
    return text.strip()


def extract_emails_regex(text: str) -> List[str]:
    """Extraction emails par regex (fallback)."""
    if not text:
        return []
    
    emails = set()
    
    # Standard
    pattern1 = r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b'
    emails.update(re.findall(pattern1, text))
    
    # Avec espaces
    pattern2 = r'\b([A-Za-z0-9._%+-]+)\s*@\s*([A-Za-z0-9.-]+)\s*\.\s*([A-Z|a-z]{2,})\b'
    for match in re.finditer(pattern2, text):
        emails.add(f"{match.group(1)}@{match.group(2)}.{match.group(3)}")
    
    # AT/DOT
    pattern3 = r'\b([A-Za-z0-9._%+-]+)\s+(?:at|AT)\s+([A-Za-z0-9.-]+)\s+(?:dot|DOT)\s+([A-Z|a-z]{2,})\b'
    for match in re.finditer(pattern3, text):
        emails.add(f"{match.group(1)}@{match.group(2)}.{match.group(3)}")
    
    # [at] [dot]
    pattern4 = r'\b([A-Za-z0-9._%+-]+)\s*\[at\]\s*([A-Za-z0-9.-]+)\s*\[dot\]\s*([A-Z|a-z]{2,})\b'
    for match in re.finditer(pattern4, text, re.IGNORECASE):
        emails.add(f"{match.group(1)}@{match.group(2)}.{match.group(3)}")
    
    cleaned = []
    for email in emails:
        email = re.sub(r'\s+', '', email).lower()
        if '@' in email and '.' in email.split('@')[1]:
            cleaned.append(email)
    
    return list(set(cleaned))


# ============================================================================
# TEST
# ============================================================================

def test_analyzer():
    """Test analyzer v1.8."""
    print("üß™ TEST ANALYZER v1.8 (avec contenu fichiers)")
    print("=" * 60)
    
    test_data = {
        'url': 'https://tanmia.ma/test',
        'organisation': 'CIDEAL',
        'titre': 'Mission √©valuation',
        'date': '2026-02-11',
        'texte_complet': "L'ALCS recherche un consultant. Contact: web@alcs.ma",
        'fichiers_attaches': [
            {
                'nom': 'TDR_Mission.pdf',
                'url': 'https://example.com/tdr.pdf',
                'type': 'pdf',
                'contenu_texte': "Termes de R√©f√©rence\nMission: √âvaluation VIH\nBudget: 50000 MAD\nDur√©e: 45 jours\nContact: tdr@alcs.ma",
                'emails_fichier': ['tdr@alcs.ma']
            }
        ],
        'emails_from_files': ['tdr@alcs.ma']
    }
    
    print("\n1. Formatage fichiers pour prompt:")
    fichiers_str = format_fichiers_for_prompt(test_data['fichiers_attaches'])
    print(fichiers_str[:500] + "...")
    
    print("\n2. Emails from files:")
    emails = get_all_emails_from_files(test_data['fichiers_attaches'])
    print(f"   {emails}")
    
    print("\n3. Fallback analysis:")
    fallback = create_fallback_analysis(test_data)
    print(f"   Emails fusionn√©s: {fallback['emails']}")
    
    print("\n‚úÖ Test termin√©")


if __name__ == "__main__":
    test_analyzer()
